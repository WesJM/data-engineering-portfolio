# Data Engineering Projects Portfolio

![Python](https://img.shields.io/badge/Python-Programming-yellow)
![SQL](https://img.shields.io/badge/SQL-Queries-lightgrey)
![API](https://img.shields.io/badge/API-Integration-blueviolet)
![Airflow](https://img.shields.io/badge/Airflow-Orchestration-blue)
![Kafka](https://img.shields.io/badge/Kafka-Streaming-black)
![dbt](https://img.shields.io/badge/dbt-Transformations-orange)
![Snowflake](https://img.shields.io/badge/Snowflake-Cloud-lightblue)
![AWS](https://img.shields.io/badge/AWS-S3%20%7C%20EC2-yellow)
![Docker](https://img.shields.io/badge/Docker-Containerization-darkblue)

Welcome! This repository highlights **end-to-end data engineering projects** I built to demonstrate skills in **real-time streaming pipelines, batch orchestration, cloud data warehousing, and modern transformation frameworks**.

Each project is fully documented and containerized for reproducibility.  

---

## üìê Projects Overview

### Real-Time NOAA Buoy Data Pipeline
- **Tech:** Kafka (Redpanda), Python, S3, Snowflake, Snowpipe, Pandas  
- **Highlights:**  
  - Real-time streaming ingestion from NOAA buoy sensors.  
  - Curated Parquet files with schema enforcement, timestamp normalization, and missing-value handling.  
  - Automated Snowflake loading via Snowpipe for analytics-ready querying.  
- üìÇ [Project Repo](https://github.com/WesJM/streaming-pipeline)  

---

### Pinball Maps Batch Data Pipeline
- **Tech:** Apache Airflow, dbt, Snowflake, Docker, REST API  
- **Highlights:**  
  - Airflow DAGs ingest data from the Pinball Maps API into Snowflake.  
  - Config-driven and data-driven DAGs with retries, logging, and error handling.  
  - dbt models structured data into a star schema (fact + dimension tables).  
- üìÇ [Project Repo](https://github.com/WesJM/batch-pipeline)  

---

## ‚ö° Skills Demonstrated
- **Streaming & Batch**: Kafka, Redpanda, Airflow  
- **Data Warehousing**: Snowflake (staging, integration, Snowpipe)  
- **Transformations**: dbt (star schema, fact/dimension modeling)  
- **Cloud & Infra**: AWS S3, Docker, docker-compose  
- **Data Quality**: Schema enforcement, timestamp normalization, missing-value handling  
- **Core Languages**: Python and SQL  

---

## üöÄ Next Steps
- Add automated data quality tests (Great Expectations/dbt tests).  
- Expand ingestion to additional APIs & buoy stations.  
- Implement CI/CD workflows for dbt + Airflow.  

---

üì´ Connect with me: [LinkedIn](https://www.linkedin.com/in/wes-martin/) | [Email](mailto:wes.martin713@gmail.com)
